{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting zip\n",
            "  Downloading zip-0.0.2.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting Flask-Admin>=1.0.4\n",
            "  Downloading Flask-Admin-1.6.0.tar.gz (6.6 MB)\n",
            "     ------------------                       3.1/6.6 MB 30.0 kB/s eta 0:01:59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Exception:\n",
            "Traceback (most recent call last):\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 437, in _error_catcher\n",
            "    yield\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 560, in read\n",
            "    data = self._fp_read(amt) if not fp_closed else b\"\"\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 526, in _fp_read\n",
            "    return self._fp.read(amt) if amt is not None else self._fp.read()\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 90, in read\n",
            "    data = self.__fp.read(amt)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\http\\client.py\", line 463, in read\n",
            "    n = self.readinto(b)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\http\\client.py\", line 507, in readinto\n",
            "    n = self.fp.readinto(b)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\socket.py\", line 704, in readinto\n",
            "    return self._sock.recv_into(b)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\ssl.py\", line 1242, in recv_into\n",
            "    return self.read(nbytes, buffer)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\ssl.py\", line 1100, in read\n",
            "    return self._sslobj.read(len, buffer)\n",
            "socket.timeout: The read operation timed out\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 160, in exc_logging_wrapper\n",
            "    status = run_func(*args)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 247, in wrapper\n",
            "    return func(self, options, args)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 400, in run\n",
            "    requirement_set = resolver.resolve(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\resolver.py\", line 92, in resolve\n",
            "    result = self._result = resolver.resolve(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 481, in resolve\n",
            "    state = resolution.resolve(requirements, max_rounds=max_rounds)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 373, in resolve\n",
            "    failure_causes = self._attempt_to_pin_criterion(name)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 213, in _attempt_to_pin_criterion\n",
            "    criteria = self._get_updated_criteria(candidate)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 204, in _get_updated_criteria\n",
            "    self._add_to_criteria(criteria, requirement, parent=candidate)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\resolvelib\\resolvers.py\", line 172, in _add_to_criteria\n",
            "    if not criterion.candidates:\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\resolvelib\\structs.py\", line 151, in __bool__\n",
            "    return bool(self._sequence)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 155, in __bool__\n",
            "    return any(self)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 143, in <genexpr>\n",
            "    return (c for c in iterator if id(c) not in self._incompatible_ids)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\found_candidates.py\", line 47, in _iter_built\n",
            "    candidate = func()\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\factory.py\", line 206, in _make_candidate_from_link\n",
            "    self._link_candidate_cache[link] = LinkCandidate(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 297, in __init__\n",
            "    super().__init__(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 162, in __init__\n",
            "    self.dist = self._prepare()\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 231, in _prepare\n",
            "    dist = self._prepare_distribution()\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\resolution\\resolvelib\\candidates.py\", line 308, in _prepare_distribution\n",
            "    return preparer.prepare_linked_requirement(self._ireq, parallel_builds=True)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 491, in prepare_linked_requirement\n",
            "    return self._prepare_linked_requirement(req, parallel_builds)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 536, in _prepare_linked_requirement\n",
            "    local_file = unpack_url(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 166, in unpack_url\n",
            "    file = get_http_url(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 107, in get_http_url\n",
            "    from_path, content_type = download(link, temp_dir.path)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\network\\download.py\", line 147, in __call__\n",
            "    for chunk in chunks:\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 53, in _rich_progress_bar\n",
            "    for chunk in iterable:\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 63, in response_chunks\n",
            "    for chunk in response.raw.stream(\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 621, in stream\n",
            "    data = self.read(amt=amt, decode_content=decode_content)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 586, in read\n",
            "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\contextlib.py\", line 137, in __exit__\n",
            "    self.gen.throw(typ, value, traceback)\n",
            "  File \"D:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\n",
            "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
            "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n"
          ]
        }
      ],
      "source": [
        "!pip install zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJ--uoEVsQVS",
        "outputId": "9274d518-c088-482c-9596-08eb6573e5dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'unzip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        }
      ],
      "source": [
        "!unzip Dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdhO5H7MuDvb"
      },
      "outputs": [],
      "source": [
        "# #50 dokumen dijadikan satu ke wadah \"data\"\n",
        "# data = []\n",
        "\n",
        "# for i in range (50):\n",
        "#   with open ('Dataset/'+str(i)+'.txt', 'r') as f:\n",
        "#     uhm =f.read()\n",
        "#     data.append(uhm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgtWW1O86Owa"
      },
      "outputs": [],
      "source": [
        "# #data yang di append dari pengulangan diatas di export\n",
        "# pd.DataFrame(data).to_csv('data.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmc_ZdFHEqCW"
      },
      "outputs": [],
      "source": [
        "# #membaca data\n",
        "# df = pd.read_csv('data.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hAkoPoWqyrb-"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "\n",
        "isi = []\n",
        "# Define the directory where the files are located\n",
        "directory = 'Dataset'\n",
        "\n",
        "# Use glob to find all .txt files in the directory\n",
        "data = glob.glob(directory + '/*.txt')\n",
        "\n",
        "# Print the names of the files\n",
        "for file in data:\n",
        "    isi.append(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "n9oD3GuxKCi1"
      },
      "outputs": [],
      "source": [
        "# Python program to\n",
        "# demonstrate merging of\n",
        "# two files\n",
        "  \n",
        "# Creating a list of filenames\n",
        "filenames = isi\n",
        "  \n",
        "# Open file3 in write mode\n",
        "with open('df.txt', 'w') as outfile:\n",
        "  \n",
        "    # Iterate through list\n",
        "    for names in filenames:\n",
        "  \n",
        "        # Open each file in read mode\n",
        "        with open(names) as infile:\n",
        "  \n",
        "            # read the data from file1 and\n",
        "            # file2 and write it in file3\n",
        "            outfile.write(infile.read())\n",
        "  \n",
        "        # Add '\\n' to enter data of file2\n",
        "        # from next line\n",
        "        outfile.write(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUtIbr6ZKqJM",
        "outputId": "fb1bcc31-ad4d-4a7a-b9c4-83c35c98774a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with open('df.txt', 'r') as file:\n",
        "    df = file.read()\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HKOgmCOULTOF"
      },
      "outputs": [],
      "source": [
        "df = df.split('.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqHA7j6dJYia",
        "outputId": "6666806f-5401-4e80-b376-fa0e7c994a43"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['']\n"
          ]
        }
      ],
      "source": [
        "df = [x.lower() for x in df]\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HilGnYYwLdHa",
        "outputId": "888d1c74-b781-44ac-9d60-b66d4316c472"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['']"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAqQu4Of5RYZ"
      },
      "outputs": [],
      "source": [
        "#bersih bersih yang kelihatan\n",
        "\n",
        "# import regex as re\n",
        "\n",
        "# def text_cleaning(input_text):\n",
        "#   text = input_text\n",
        "#   text = re.sub(r\"ir\\. h\\.\",\"ir h\",text)\n",
        "#   text = text.lower()\n",
        "#   return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_E8wzIPIrwV",
        "outputId": "dc72444f-6399-4659-ee1a-e492aa7ad5de"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "empty vocabulary; perhaps the documents only contain stop words",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      2\u001b[0m otak \u001b[39m=\u001b[39m TfidfVectorizer()\n\u001b[1;32m----> 3\u001b[0m otak\u001b[39m.\u001b[39;49mfit(df)\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2091\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2084\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_warn_for_unused_params()\n\u001b[0;32m   2085\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2086\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2087\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2088\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2089\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2090\u001b[0m )\n\u001b[1;32m-> 2091\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2092\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2093\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1377\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1369\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1370\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1371\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1372\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1373\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1374\u001b[0m             )\n\u001b[0;32m   1375\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[0;32m   1379\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[0;32m   1380\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1283\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1281\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1282\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1283\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1284\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1285\u001b[0m         )\n\u001b[0;32m   1287\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n",
            "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "otak = TfidfVectorizer()\n",
        "otak.fit(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IRHckxUpFtBO",
        "outputId": "1f8c5460-bebd-40a0-d301-d3c62d06b712"
      },
      "outputs": [
        {
          "ename": "NotFittedError",
          "evalue": "Vocabulary not fitted or provided",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNotFittedError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m v_kalimat \u001b[39m=\u001b[39m otak\u001b[39m.\u001b[39;49mtransform(df)\n\u001b[0;32m      2\u001b[0m v_kalimat \u001b[39m=\u001b[39m v_kalimat\u001b[39m.\u001b[39mtoarray()\n\u001b[0;32m      3\u001b[0m v_kalimat\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2145\u001b[0m, in \u001b[0;36mTfidfVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   2128\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Transform documents to document-term matrix.\u001b[39;00m\n\u001b[0;32m   2129\u001b[0m \n\u001b[0;32m   2130\u001b[0m \u001b[39mUses the vocabulary and document frequencies (df) learned by fit (or\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2141\u001b[0m \u001b[39m    Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   2142\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   2143\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m, msg\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe TF-IDF vectorizer is not fitted\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m-> 2145\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mtransform(raw_documents)\n\u001b[0;32m   2146\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mtransform(X, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1419\u001b[0m, in \u001b[0;36mCountVectorizer.transform\u001b[1;34m(self, raw_documents)\u001b[0m\n\u001b[0;32m   1415\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(raw_documents, \u001b[39mstr\u001b[39m):\n\u001b[0;32m   1416\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1417\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIterable over raw text documents expected, string object received.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1418\u001b[0m     )\n\u001b[1;32m-> 1419\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_vocabulary()\n\u001b[0;32m   1421\u001b[0m \u001b[39m# use the same matrix-building strategy as fit_transform\u001b[39;00m\n\u001b[0;32m   1422\u001b[0m _, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_count_vocab(raw_documents, fixed_vocab\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
            "File \u001b[1;32md:\\Anaconda\\envs\\opencv-env\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:510\u001b[0m, in \u001b[0;36m_VectorizerMixin._check_vocabulary\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    508\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_vocabulary()\n\u001b[0;32m    509\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfixed_vocabulary_:\n\u001b[1;32m--> 510\u001b[0m         \u001b[39mraise\u001b[39;00m NotFittedError(\u001b[39m\"\u001b[39m\u001b[39mVocabulary not fitted or provided\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    512\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvocabulary_) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    513\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mVocabulary is empty\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[1;31mNotFittedError\u001b[0m: Vocabulary not fitted or provided"
          ]
        }
      ],
      "source": [
        "v_kalimat = otak.transform(df)\n",
        "v_kalimat = v_kalimat.toarray()\n",
        "v_kalimat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUE3b2h-FHAF",
        "outputId": "8c261b73-36bb-40b9-f411-37dfa4cdabe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Masukan keyword:semarang\n",
            " kedatangan kartsen di semarang adalah guna merancang kota semarang dan kota-kota lainnya di pulau jawa, termasuk purwokerto\n"
          ]
        }
      ],
      "source": [
        "cari = input(\"Masukan keyword:\")\n",
        "v_cari = otak.transform([cari]).toarray()\n",
        "indeks = 0\n",
        "hasil = 0\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "for i,v in enumerate(v_kalimat):\n",
        "  mirip= cosine_similarity([v],v_cari)\n",
        "  if mirip > hasil:\n",
        "    hasil = mirip\n",
        "    indeks = i\n",
        "\n",
        "print(df[indeks])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "opencv-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13 (main, Oct 13 2022, 21:23:06) [MSC v.1916 64 bit (AMD64)]"
    },
    "vscode": {
      "interpreter": {
        "hash": "3770fde709983dcfa19c7f5d603a4d20143aadb2c4ed2f9ee3ef30d7a7293147"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
